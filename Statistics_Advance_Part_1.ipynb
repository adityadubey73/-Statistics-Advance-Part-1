{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # THEORY\n",
        "# What is a random variable in probability theory\n",
        "\n",
        "A random variable in probability theory is a variable whose value is a numerical outcome of a random phenomenon. It's a function that maps the outcomes of a random process to a numerical value.\n",
        "\n"
      ],
      "metadata": {
        "id": "kxwZnscAX9r6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the types of random variables\n",
        "\n",
        "Discrete Random Variables:\n",
        "Definition:\n",
        "A discrete random variable can be counted or listed out. It can only take on specific, separate values.\n",
        "\n",
        "Examples:\n",
        "The number of heads when flipping a coin three times (could be 0, 1, 2, or 3).\n",
        "The number of cars that pass a certain point on a highway in one hour.\n",
        "The roll of a die (1, 2, 3, 4, 5, or 6).\n",
        "The number of defective items in a batch of 20.\n",
        "Continuous Random Variables:\n",
        "\n",
        "Definition:\n",
        "A continuous random variable can take on any value within a given range or interval.\n",
        "\n",
        "Examples:\n",
        "The height of a student.\n",
        "The temperature of a room.\n",
        "The exact time it takes to complete a task.\n",
        "The amount of rainfall in a year.\n"
      ],
      "metadata": {
        "id": "WmI5C4bdZDZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the difference between discrete and continuous distributions9\n",
        "\n",
        "> Discrete Distributions:\n",
        "Countable Values:\n",
        "Discrete distributions represent data that can only take on certain specific, distinct values, often whole numbers. These values are often countable, and there are gaps between them.\n",
        "\n",
        "Examples:\n",
        "The number of heads when flipping a coin a few times, the number of customers in a queue, or the number of defects in a manufactured product are examples of discrete data.\n",
        "\n",
        "Probability Mass Function (PMF):\n",
        "In discrete distributions, probabilities are assigned to specific values, forming a probability mass function.\n",
        "\n",
        "Continuous Distributions:\n",
        "Values within a Range:\n",
        "Continuous distributions deal with data that can take on any value within a specified range. There are no gaps between possible values.\n",
        "\n",
        "Examples:\n",
        "Height, weight, temperature, or time are examples of continuous data.\n",
        "\n",
        "Probability Density Function (PDF):\n",
        "In continuous distributions, instead of assigning probabilities to specific values, we have a probability density function. This function describes the likelihood of values falling within a certain range.\n",
        "\n",
        "Probability of a Specific Value:\n",
        "Theprobability of a continuous variable taking on a specific value is technically zero because of the infinite number of possibilities within a range. However, we can still calculate the probability of a value falling within a certain range.\n"
      ],
      "metadata": {
        "id": "lLvz-9OsZjP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are probability distribution functions (PDF)\n",
        "\n",
        "> A Probability Density Function (PDF) describes the likelihood of a continuous random variable falling within a specific range of values. It's a function that represents the density of probability rather than directly giving probabilities.\n"
      ],
      "metadata": {
        "id": "G3xS31dIaAXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do cumulative distribution functions (CDF) differ from probability distribution functions (PDF)9\n",
        "\n",
        "> Probability Distribution Function (PDF):\n",
        "Describes the likelihood of a continuous random variable taking on a specific value.\n",
        "\n",
        "The area under the PDF curve between two points represents the probability that the random variable falls within that range.\n",
        "The total area under the PDF curve is always equal to 1, as it represents the total probability of all possible values.\n",
        "\n",
        "* Cumulative Distribution Function (CDF):\n",
        "Provides the probability that a random variable is less than or equal to a specific value.\n",
        "\n",
        "The CDF is a non-decreasing function that starts at 0 and increases to 1 as the value increases.\n",
        "The CDF can be obtained by integrating the PDF from negative infinity to the value of interest."
      ],
      "metadata": {
        "id": "eVS1dQtpbJGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  What is a discrete uniform distribution\n",
        "\n",
        "> A discrete uniform distribution is a probability distribution where a finite number of outcomes are all equally likely. Think of rolling a fair six-sided die; each number (1 through 6) has a probability of 1/6 of being rolled, making it a discrete uniform distribution.\n"
      ],
      "metadata": {
        "id": "F_TblE_kbZT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the key properties of a Bernoulli distribution9\n",
        "\n",
        "> Binary Outcomes: The random variable in a Bernoulli distribution can only take two values: 1 (success) or 0 (failure).\n",
        "\n",
        "Constant Probability: The probability of success, denoted by p, remains the same for each trial.\n",
        "\n",
        "Independent Trials: The outcome of one trial does not affect the outcome of any other trial.\n",
        "\n",
        "Complementary Probability: The probability of failure is denoted by q and is equal to 1 - p.\n",
        "Mean: The expected value (mean) of a Bernoulli random variable is equal to p.\n",
        "\n",
        "Variance: The variance, which measures the spread of the distribution, is calculated as p(1-p) or p q."
      ],
      "metadata": {
        "id": "1k04yo5nbil9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the binomial distribution, and how is it used in probability\n",
        "\n",
        "> The binomial distribution is a probability distribution that models the probability of obtaining a specific number of successes in a fixed number of independent trials, where each trial has only two possible outcomes (success or failure) and the probability of success is constant across all trials\n",
        "\n",
        "Independent Trials: The outcome of one trial does not affect the outcome of other trials.\n",
        "\n",
        "Fixed Number of Trials (n): The binomial distribution requires a predetermined number of trials.\n",
        "Two Possible Outcomes: Each trial results in either success or failure.\n",
        "\n",
        "Constant Probability of Success (p): The probability of success remains the same for every trial.\n",
        "\n",
        "Formula:\n",
        "The probability of exactly k successes in n trials is given by:\n",
        "P(X = k) = (n choose k) * p^k * (1-p)^(n-k)\n"
      ],
      "metadata": {
        "id": "ZTFt0Q8ab4yW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the Poisson distribution and where is it applied\n",
        "\n",
        "> The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. It's used when you're dealing with rare events that happen independently at a constant average rate within a specific interval."
      ],
      "metadata": {
        "id": "XhhvVn9fcsBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is a continuous uniform distribution\n",
        "\n",
        "> A continuous uniform distribution is a probability distribution where a continuous random variable is equally likely to take any value within a specified range (a, b). This means the probability of the variable falling within any sub-interval of the same width is the same.\n",
        "\n"
      ],
      "metadata": {
        "id": "7ZnsXkoXdE-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the characteristics of a normal distribution\n",
        "\n",
        "> A normal distribution, often called a bell curve, has several key characteristics: it's symmetric, unimodal (has one peak), and asymptotic (its tails approach but never touch the x-axis). The mean, median, and mode are all equal and located at the center, which is the peak of the curve. The shape is defined by the mean and standard deviation.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "Symmetry:\n",
        "The distribution is perfectly symmetrical around its mean. If you fold the curve in half at the mean, the two sides would perfectly overlap.\n",
        "\n",
        "Bell Shape:\n",
        "The curve has a characteristic bell shape, with the highest point at the mean and tapering off towards the tails.\n",
        "\n",
        "Mean, Median, and Mode are Equal:\n",
        "In a normal distribution, the average (mean), the middle value (median), and the most frequent value (mode) are all the same and located at the center.\n",
        "\n",
        "Standard Deviation:\n",
        "The standard deviation determines the width of the curve. A smaller standard deviation means the data is more concentrated around the mean, resulting in a narrower, taller curve. A larger standard deviation means the data is more spread out, resulting in a wider, flatter curve.\n",
        "\n",
        "Unimodal:\n",
        "The curve has only one peak.\n",
        "Asymptotic:\n",
        "The tails of the curve extend infinitely but never touch the horizontal axis.\n"
      ],
      "metadata": {
        "id": "UiGHdHQcdUoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the standard normal distribution, and why is it important\n",
        "\n",
        "> The standard normal distribution is a specific type of normal distribution with a mean of 0 and a standard deviation of 1. It's a crucial concept in statistics because it allows for the comparison of data from different normal distributions and enables probability calculations using z-scores. By converting data into z-scores (number of standard deviations from the mean), you can use a standard normal table to find probabilities associated with specific values.\n",
        "\n",
        "Why is it Important?\n",
        "1. Simplifies Probability Calculations:\n",
        "By converting any normal distribution to the standard normal distribution (using z-scores), you can use a single standard normal table to find probabilities.\n",
        "\n",
        "2. Enables Comparisons:\n",
        "It allows for comparison of data from different normal distributions with different means and standard deviations.\n",
        "\n",
        "3. Foundation for Statistical Inference:\n",
        "Many statistical methods rely on the standard normal distribution, making it a cornerstone of statistical analysis.\n",
        "\n",
        "4. Predictive Power:\n",
        "The standard normal distribution, especially when combined with the central limit theorem, is useful for making predictions and informed decisions.\n",
        "\n",
        "5. Approximation of Other Distributions:\n",
        "Many real-world phenomena can be approximated by the normal distribution, making the standard normal distribution a valuable tool for understanding and modeling these phenomena."
      ],
      "metadata": {
        "id": "wdsYSbCRd4ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the Central Limit Theorem (CLT), and why is it critical in statistics\n",
        "\n",
        "> The Central Limit Theorem (CLT) states that the distribution of sample means will approach a normal distribution, regardless of the original population distribution, as the sample size becomes sufficiently large. This is critical in statistics because it allows us to use the normal distribution and its associated properties to make inferences about populations, even when we don't know the population's true distribution.\n",
        "\n",
        "Here's why it's so important:\n",
        "Enables Statistical Inference:\n",
        "The CLT is the foundation for many statistical procedures, like hypothesis testing and confidence interval estimation, which rely on the normal distribution.\n",
        "\n",
        "Simplifies Analysis:\n",
        "Many statistical tests and models assume data is normally distributed. The CLT allows us to apply these methods to a wider range of datasets, even those that aren't inherently normal, as long as the sample size is large enough.\n",
        "\n",
        "Real-world Applications:\n",
        "The CLT has numerous practical applications. For example, in quality control, it helps monitor manufacturing processes; in finance, it aids in analyzing stock returns.\n",
        "\n",
        "Foundation for Machine Learning:\n",
        "The CLT also underpins many machine learning algorithms and techniques, such as model validation and resampling methods\n"
      ],
      "metadata": {
        "id": "xIaHHSrFeUVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does the Central Limit Theorem relate to the normal distribution\n",
        "\n",
        "> The central limit theorem (CLT) essentially states that the distribution of sample means will approximate a normal distribution, regardless of the underlying population distribution, as long as the sample size is sufficiently large. This means that even if you are sampling from a population that is not normally distributed, the averages of multiple samples will tend towards a normal distribution\n",
        "\n",
        "Normal Distribution:\n",
        "The normal distribution, also known as the Gaussian distribution, is a bell-shaped probability distribution characterized by its mean and standard deviation.\n",
        "\n",
        "Central Limit Theorem:\n",
        "The CLT focuses on the distribution of sample means. It states that if you take many random samples from a population and calculate the mean of each sample, the distribution of these sample means will be approximately normal, even if the original population isn't normally distributed.\n",
        "\n",
        "Relationship:\n",
        "The CLT's significance lies in its connection to the normal distribution. It allows us to use the properties of the normal distribution (like probabilities associated with specific ranges) to analyze and make inferences about sample means, even when the population distribution is unknown or non-normal.\n",
        "\n",
        "Sample Size:\n",
        "The larger the sample size, the better the approximation of the sample means to a normal distribution. A common rule of thumb is that a sample size of 30 or more is generally considered \"sufficiently large\" for the CLT to apply, but this can vary depending on the shape of the original population distribution."
      ],
      "metadata": {
        "id": "U1yuUl5pe5d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the application of Z statistics in hypothesis testing\n",
        "\n",
        "> Z-statistics are used in hypothesis testing to determine if there's a significant difference between a sample mean and a population mean, or between the means of two samples, especially when the population variance is known or the sample size is large. They help assess the likelihood that observed differences are statistically significant, rather than due to random chance.\n",
        "\n",
        "Here's a breakdown of Z-statistics in hypothesis testing:\n",
        "1. Understanding the Basics:\n",
        "Z-test:\n",
        "A Z-test is a statistical test that utilizes the Z-statistic to determine if there is a significant difference between a sample mean and a population mean, or between the means of two samples.\n",
        "\n",
        "Z-statistic:\n",
        "It's a measure that indicates how far away a sample mean is from the population mean, expressed in standard deviations.\n",
        "Large Sample Size:\n",
        "Z-tests are typically used when dealing with large sample sizes (often n ≥ 30), or when the population standard deviation is known.\n",
        "Population Variance:\n",
        "Z-tests are appropriate when the population variance is known or can be reliably estimated from the sample data.\n",
        "\n",
        "2. Applications in Hypothesis Testing:\n",
        "One-sample Z-test:\n",
        "This test compares a sample mean to a known population mean. For example, testing if the average height of students in a particular school is significantly different from the average height of all students in the country.\n",
        "Two-sample Z-test:\n",
        "This test compares the means of two independent samples. For instance, comparing the average test scores of students in two different schools.\n",
        "Two-sample Paired Z-test:\n",
        "This test is used when comparing two related sets of data (e.g., before and after measurements on the same individuals).\n",
        "Testing Proportions:\n",
        "Z-tests can also be used to compare a sample proportion to a known population proportion or to compare proportions between two samples.\n",
        "\n",
        "3. How Z-tests are used in hypothesis testing:\n",
        "Formulate hypotheses:\n",
        "Define the null and alternative hypotheses (e.g., null hypothesis: there is no significant difference between the sample mean and population mean).\n",
        "Set a significance level (α):\n",
        "Determine the level of significance, which defines the threshold for rejecting the null hypothesis.\n",
        "\n",
        "Calculate the Z-statistic:\n",
        "Use the appropriate formula for the chosen Z-test (one-sample, two-sample, etc.).\n",
        "Determine the critical value or p-value:\n",
        "Compare the calculated Z-statistic to the critical value from the Z-table (based on the significance level and test type - one-tailed or two-tailed) or find the p-value.\n",
        "\n",
        "Make a decision:\n",
        "If the calculated Z-statistic falls in the rejection region (beyond the critical value) or the p-value is less than α, reject the null hypothesis.\n",
        "\n",
        "4. Key Considerations:\n",
        "Normality Assumption:\n",
        "Z-tests assume that the data is normally distributed. However, with large sample sizes, the Central Limit Theorem allows for the use of Z-tests even if the data is not perfectly normal.\n",
        "\n",
        "Variance:\n",
        "It's crucial to know the population variance or have a large sample size for accurate results.\n",
        "\n",
        "Choosing the right test:\n",
        "Carefully consider whether a one-sample or two-sample Z-test is appropriate for the research question.\n",
        "\n",
        "Interpreting results:\n",
        "Understand that statistical significance doesn't always equate to practical significance.\n",
        "Z-Test: Formula, Examples, Uses, Z-Test vs T-Test\n"
      ],
      "metadata": {
        "id": "XPkXO6_Uf6qs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  How do you calculate a Z-score, and what does it represent\n",
        "\n",
        "> A z-score represents how many standard deviations a data point is away from the mean of a dataset. It's calculated by subtracting the mean from the data point and dividing the result by the standard deviation. A positive z-score indicates the data point is above the mean, while a negative score indicates it's below the mean.\n",
        "\n",
        "Calculation:\n",
        "The formula for calculating a z-score is:\n",
        "z = (x - μ) / σ\n",
        "Where: z is the z-score, x is the individual data point, μ is the population mean, and σ is the population standard deviation.\n"
      ],
      "metadata": {
        "id": "9bJ0kNajgc5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  What are point estimates and interval estimates in statistics\n",
        "\n",
        "> Point Estimates:\n",
        "A point estimate is a single value calculated from sample data that is used to estimate a population parameter.\n",
        "For example, the sample mean is a point estimate of the population mean.\n",
        "Point estimates are straightforward to calculate and understand, but they do not provide any information about the precision or reliability of the estimate.\n",
        "A point estimate alone doesn't tell you how close it is to the true population value.\n",
        "\n",
        "* Interval Estimates:\n",
        "An interval estimate provides a range of values within which the population parameter is likely to fall, along with a specified level of confidence.\n",
        "\n",
        "A common type of interval estimate is the confidence interval, which gives a range of values that is likely to contain the true population parameter.\n",
        "\n",
        "Interval estimates are more informative than point estimates because they provide information about the uncertainty associated with the estimate.\n",
        "\n",
        "For instance, a 95% confidence interval means that if you were to repeat the sampling process many times, 95% of the calculated confidence intervals would contain the true population parameter.\n",
        "\n",
        "Interval estimates are often preferred over point estimates because they provide a more complete picture of the estimation process by quantifying the uncertainty.\n"
      ],
      "metadata": {
        "id": "AhcZLfn6helj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the significance of confidence intervals in statistical analysis\n",
        "\n",
        "> Confidence intervals in statistical analysis provide a range of plausible values for an unknown population parameter, indicating the precision of an estimate derived from a sample. They offer a more informative measure of uncertainty than a single point estimate and help in generalizing findings from a sample to a larger population. A 95% confidence interval, for example, suggests that if the study were repeated multiple times, 95% of the calculated intervals would contain the true population parameter."
      ],
      "metadata": {
        "id": "luoLgz_fh-II"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the relationship between a Z-score and a confidence interval\n",
        "\n",
        "> Z-score:\n",
        "A Z-score indicates how many standard deviations a data point is from the mean of a distribution.\n",
        "In the context of confidence intervals, the Z-score is used to determine the width of the interval, representing the level of confidence.\n",
        "For example, a 95% confidence interval typically uses a Z-score of 1.96, meaning 95% of the data falls within 1.96 standard deviations of the mean.\n",
        "\n",
        "* Confidence Interval:\n",
        "A confidence interval provides a range of values within which the true population parameter is likely to fall, based on a sample.\n",
        "It is calculated by taking the sample mean and adding and subtracting a margin of error.\n",
        "The margin of error is calculated using the Z-score, standard deviation, and sample size.\n"
      ],
      "metadata": {
        "id": "DXLuF6YWiMkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How are Z-scores used to compare different distributions\n",
        "\n",
        "> Z-scores are used to compare data from different distributions by standardizing the values. They represent how many standard deviations a data point is from its distribution's mean, allowing for direct comparison even if the distributions have different means and standard deviations.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "How Z-scores work:\n",
        "1. Standardization:\n",
        "Z-scores transform raw data into a standard scale where the mean is 0 and the standard deviation is 1. This is done by calculating: z = (x - μ) / σ, where x is the data point, μ is the mean of the distribution, and σ is the standard deviation.\n",
        "\n",
        "2. Comparison:\n",
        "By converting data points to z-scores, you can compare values from different distributions on a common scale. For example, a z-score of +2 in one distribution means the same thing as a z-score of +2 in another distribution: it represents a value that is two standard deviations above the mean of its respective distribution.\n",
        "\n",
        "3. Meaningful Interpretation:\n",
        "Z-scores indicate the relative position of a data point within its distribution. A positive z-score indicates a value above the mean, while a negative z-score indicates a value below the mean. The larger the absolute value of the z-score, the more extreme the data point is relative to its distribution.\n",
        "\n",
        "Example:\n",
        "Imagine comparing the test scores of two students who took different exams with different average scores and standard deviations. By converting their scores to z-scores, you can see who performed better relative to their respective class. For example, if one student has a z-score of 1.5 and the other has a z-score of 1.0, the student with the z-score of 1.5 performed better relative to their class, even if their raw scores were different.\n",
        "\n",
        "Key Advantages:\n",
        "Comparability:\n",
        "Z-scores allow for meaningful comparisons between distributions with different means and standard deviations"
      ],
      "metadata": {
        "id": "cojbRzQsizKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the assumptions for applying the Central Limit Theorem\n",
        "\n",
        "> The central limit theorem (CLT) allows us to assume that the distribution of sample means will be approximately normal, regardless of the population's distribution, as long as the sample size is sufficiently large. This is crucial for many statistical inferences. The main assumptions are: random sampling, independence of samples, and sufficiently large sample size.\n",
        "\n",
        "Here's a breakdown:\n",
        "Random Sampling:\n",
        "The samples must be selected randomly from the population to ensure they are representative.\n",
        "\n",
        "Independence:\n",
        "Individual observations within the sample and across different samples should be independent of each other. This means one sample point shouldn't influence another.\n",
        "\n",
        "Sufficiently Large Sample Size:\n",
        "The sample size should be large enough for the CLT to hold. Generally, a sample size of 30 or more is considered sufficient, but this can vary depending on the population distribution. For highly skewed distributions, a larger sample size might be needed.\n",
        "\n",
        "Finite Variance:\n",
        "The population from which the samples are drawn should have a finite variance. This is usually not a problem in practice, as most real-world populations have finite variance.\n",
        "Central limit theorem - Wikipedia\n",
        "In probability theory, the central limit theorem (CLT) states that, under appropriate conditions, the distribution of a normalized...\n",
        "\n"
      ],
      "metadata": {
        "id": "j9nKQvM2jVQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the concept of expected value in a probability distribution\n",
        "\n",
        ". The expected value in a probability distribution represents the long-run average outcome of a random variable. It's a weighted average of all possible values, where each value is weighted by its corresponding probability. Essentially, it tells you what you can \"expect\" to happen on average if you repeat an experiment many times.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "Probability Distribution:\n",
        "A probability distribution describes the likelihood of each possible outcome of a random variable.\n",
        "\n",
        "Random Variable:\n",
        "A random variable is a variable whose value is a numerical outcome of a random phenomenon.\n",
        "\n",
        "Expected Value (E(X)):\n",
        "For a discrete random variable, the expected value is calculated by multiplying each possible outcome by its probability and summing the results. The formula is: E(X) = Σ [x * P(x)], where 'x' represents each outcome and 'P(x)' its probability.\n",
        "\n",
        "Interpreting Expected Value:\n",
        "The expected value is not necessarily a value that will be observed in any single trial of the experiment. Instead, it represents the average outcome over many repetitions. If you were to repeatedly run the experiment, the average of the outcomes would tend towards the expected value.\n"
      ],
      "metadata": {
        "id": "hQHBeDYsjsge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does a probability distribution relate to the expected outcome of a random variable\n",
        "\n",
        "> Probability Distribution:\n",
        "For a random variable, the probability distribution specifies the probability of each possible value the variable can take. For a discrete random variable, this is a probability mass function (PMF), and for a continuous random variable, it's a probability density function (PDF).\n",
        "\n",
        "Expected Value (Mean):\n",
        "The expected value (often denoted E[X] or μ) represents the long-run average outcome of a random variable. For a discrete random variable, it's calculated by summing the product of each outcome and its probability. For a continuous random variable, it involves an integral.\n",
        "\n",
        "Relationship:\n",
        "The probability distribution provides the foundation for calculating the expected value. It tells you the likelihood of each outcome, which is then used in the calculation of the weighted average (expected value). Essentially, the expected value is a summary statistic that captures the central tendency of the probability distribution."
      ],
      "metadata": {
        "id": "5aYNZm2Wj90r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "hC7UB0iokR-9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BYdtcDjiYiQs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}